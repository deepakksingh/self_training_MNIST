import matplotlib.pyplot as plt

number_of_training_images = [250,500,750,1000,1250,1500,1750,2000,2250,2500,2750]
baseline_accuracy_values = [0.790333,0.82739,0.847402,0.849417,0.860702,0.874632,0.873692,0.881758,0.879404,0.882933,0.885476]
self_training_exp_1_leastloss = [0.744183,0.780512,0.792553,0.823469,0.832973,0.83816,0.857647,0.863955,0.868391,0.881465,0.882089] 
self_training_exp_1_highestloss = [0.753801,0.821415,0.837599,0.856305,0.86354,0.875054,0.872691,0.877032,0.876449,0.882315,0.884406] 

#epoch:10 and batch_size: 2000
batch_2k_x_axis = []
asce_b2k_e10 = [0.9083, 0.9402, 0.9559, 0.9641, 0.9684, 0.9713, 0.9749, 0.976, 0.9774, 0.9795, 0.9797, 0.9795, 0.9803, 0.9815, 0.9804, 0.98, 0.9806, 0.9805, 0.9809, 0.9807, 0.9815, 0.9825, 0.9822, 0.9825]
desc_b2k_e10 = [0.906, 0.9304, 0.934, 0.9434, 0.9452, 0.9406, 0.946, 0.9471, 0.9444, 0.943, 0.9464, 0.9461, 0.9445, 0.9471, 0.9478, 0.9405, 0.9411, 0.9456, 0.9458, 0.9474, 0.9465, 0.9483, 0.929, 0.9464]
rand_b2k_e10 = [0.9053, 0.935, 0.9486, 0.9562, 0.9636, 0.9633, 0.9644, 0.9643, 0.964, 0.9627, 0.9657, 0.9672, 0.9667, 0.9673, 0.966, 0.9651, 0.9667, 0.9657, 0.9654, 0.9658, 0.9675, 0.8141, 0.9671, 0.9088]


#epoch:10 and batch_size: 5000
batch_5k_x_axis = []
asce_b5k_e10 = [0.9098, 0.9406, 0.952, 0.9619, 0.9653, 0.971, 0.9726, 0.9746, 0.9764]
desc_b5k_e10 = [0.8994, 0.9061, 0.9197, 0.9234, 0.9244, 0.9239, 0.9204, 0.9207, 0.9257]
rand_b5k_e10 = [0.9128, 0.9423, 0.9519, 0.9542, 0.9557, 0.9561, 0.958, 0.9581, 0.9565]

#epoch:10 and batch_size: 1000
batch_1k_x_axis = []
asce_b1k_e10 = [0.9112, 0.9385, 0.9549, 0.962, 0.9682, 0.9712, 0.9745, 0.976, 0.9777, 0.9781, 0.9791, 0.9801, 0.9808, 0.9797, 0.9814, 0.9811, 0.9813, 0.9818, 0.9814, 0.9822, 0.982, 0.982, 0.9828, 0.9826, 0.9828, 0.983, 0.9829, 0.9829, 0.9824, 0.9826, 0.9835, 0.9828, 0.9832, 0.9836, 0.9848, 0.9846, 0.9847, 0.9843, 0.9844, 0.9834, 0.984, 0.984, 0.9836, 0.984, 0.9841, 0.9845, 0.9846, 0.9845]
desc_b1k_e10 = [0.915, 0.9387, 0.9445, 0.9454, 0.9502, 0.9469, 0.9506, 0.9519, 0.9538, 0.9502, 0.9513, 0.9522, 0.9517, 0.9539, 0.9512, 0.9533, 0.953, 0.9529, 0.9541, 0.9505, 0.9517, 0.9522, 0.9512, 0.9491, 0.9533, 0.9503, 0.9496, 0.952, 0.9523, 0.9527, 0.9527, 0.9527, 0.9478, 0.9518, 0.9538, 0.9481, 0.9515, 0.947, 0.9537, 0.9505, 0.9521, 0.945, 0.9522, 0.9512, 0.9495, 0.9513, 0.8806, 0.94]
rand_b1k_e10 = [0.912, 0.9421, 0.955, 0.9617, 0.9666, 0.968, 0.9715, 0.9723, 0.9725, 0.9739, 0.9744, 0.9747, 0.9752, 0.9744, 0.9754, 0.9755, 0.9755, 0.9757, 0.9763, 0.9753, 0.9762, 0.9764, 0.9755, 0.9464, 0.976, 0.9758, 0.9748, 0.9746, 0.9759, 0.9744, 0.9753, 0.9732, 0.9748, 0.9748, 0.975, 0.9748, 0.9758, 0.9754, 0.9765, 0.9752, 0.9739, 0.9737, 0.9744, 0.9747, 0.9747, 0.9741, 0.9745, 0.9744]

#epoch:250 and batch_size: 5000
batch_5k_x_axis = []
asce_b5k_e250 = [0.9852, 0.985, 0.9854, 0.9855, 0.9855, 0.9855, 0.9855, 0.9854, 0.9855]
desc_b5k_e250 = [0.9822, 0.9823, 0.9818, 0.9818, 0.9818, 0.9815, 0.9814, 0.9813, 0.9819]
rand_b5k_e250 = [0.9834, 0.9836, 0.984, 0.9837, 0.9837, 0.9841, 0.9841, 0.9841, 0.984]

#epoch:250 and batch_size: 2000
batch_2k_x_axis = []
asce_b2k_e250 = [0.9831, 0.9845, 0.9843, 0.9851, 0.987, 0.9856, 0.9857, 0.9855, 0.9854, 0.9854, 0.9856, 0.9856, 0.9855, 0.9856, 0.9856, 0.9856, 0.9856, 0.9857, 0.9857, 0.9857, 0.9855, 0.9857, 0.9856, 0.9856]
desc_b2k_e250 = [0.9809, 0.9806, 0.9815, 0.9816, 0.9815, 0.9815, 0.9814, 0.9816, 0.9816, 0.9815, 0.9816, 0.9816, 0.9816, 0.9816, 0.9816, 0.9816, 0.9815, 0.9815, 0.9814, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813]
rand_b2k_e250 = [0.9834, 0.9841, 0.9846, 0.9848, 0.9843, 0.9844, 0.9845, 0.9846, 0.9845, 0.9844, 0.9844, 0.9844, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842, 0.9842]

fig, ax = plt.subplots()

line1, = ax.plot(number_of_training_images, baseline_accuracy_values , 
                marker='o', 
                color='b',
                label='baseline with 500 val images')

line2, = ax.plot(number_of_training_images, self_training_exp_1_leastloss , 
marker='^', 
color='g',
label='self-training with diminishing surplus (least loss)')

line3, = ax.plot(number_of_training_images, self_training_exp_1_highestloss , 
marker='s', 
color='r',
label='self-training with diminishing surplus (highest loss)')

plt.xticks(number_of_training_images)
plt.xlabel("Number of training images")
plt.ylabel("Accuracy")
ax.legend()
plt.show()